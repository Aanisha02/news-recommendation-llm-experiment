#Code generated by ChatGPT, OpenAI, November 11, 2025, https://chat.openai.com/chat

import os
from pathlib import Path

import numpy as np
import torch
from torch.utils.data import DataLoader
from transformers import AutoConfig
from transformers.modeling_outputs import ModelOutput

from const.path import MIND_SMALL_VAL_DATASET_DIR
from evaluation.RecEvaluator import RecEvaluator, RecMetrics
from mind.dataframe import read_behavior_df, read_news_df
from mind.MINDDataset import MINDValDataset
from recommendation.nrms import NRMS, PLMBasedNewsEncoder, UserEncoder
from utils.logger import logging
from utils.text import create_transform_fn_from_pretrained_tokenizer

# -------------------------------------------------
# CONFIG â€“ change CKPT_DIR if your best model is elsewhere
# -------------------------------------------------

# This should point to your trained checkpoint directory
# (the folder that contains pytorch_model.bin)
CKPT_DIR = Path("output/2025-11-16/19-53-18/checkpoint-1000")
PRETRAINED_NAME = "distilbert-base-uncased"
MAX_LEN = 20
HISTORY_SIZE = 20  # same as you used in training
EVAL_BATCH_SIZE = 1
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# -------------------------------------------------
# Helpers
# -------------------------------------------------


def load_model() -> NRMS:
    """Recreate NRMS architecture and load fine-tuned weights."""
    logging.info(f"Loading model from: {CKPT_DIR}")

    hidden_size: int = AutoConfig.from_pretrained(PRETRAINED_NAME).hidden_size
    news_encoder = PLMBasedNewsEncoder(PRETRAINED_NAME)
    user_encoder = UserEncoder(hidden_size=hidden_size)

    net = NRMS(
        news_encoder=news_encoder,
        user_encoder=user_encoder,
        hidden_size=hidden_size,
        loss_fn=None,
    ).to(DEVICE)

    state_path = CKPT_DIR / "pytorch_model.bin"
    state_dict = torch.load(state_path, map_location=DEVICE)
    net.load_state_dict(state_dict)
    net.eval()
    return net


def ratio_at_k(candidate_labels, scores, k: int) -> float:
    """
    Ratio@K: fraction of REAL (label=1) items among the top-K ranked candidates.
    """
    candidate_labels = np.array(candidate_labels)
    scores = np.array(scores)

    if len(candidate_labels) == 0:
        return np.nan

    k = min(k, len(scores))
    top_idx = np.argsort(scores)[::-1][:k]
    top_labels = candidate_labels[top_idx]
    # label=1 => real, label=0 => fake
    return float(top_labels.mean())


def average_rec_metrics(metrics_list):
    if not metrics_list:
        return None
    return RecMetrics(
        **{
            "ndcg_at_10": float(
                np.average([m.ndcg_at_10 for m in metrics_list])
            ),
            "ndcg_at_5": float(
                np.average([m.ndcg_at_5 for m in metrics_list])
            ),
            "auc": float(np.average([m.auc for m in metrics_list])),
            "mrr": float(np.average([m.mrr for m in metrics_list])),
        }
    )


def rel_adv(hr: float, hf: float) -> float | None:
    """
    Relative Ranking Advantage (RRA) in %: (HR - HF) / HF * 100
    """
    if hf is None or hr is None or hf == 0:
        return None
    return (hr - hf) / hf * 100.0


# -------------------------------------------------
# Main computation
# -------------------------------------------------


def compute_metrics_table():
    # 1) Load model
    model = load_model()

    # 2) Load news + behaviors for validation split
    logging.info("Loading validation data...")

    val_news_path = MIND_SMALL_VAL_DATASET_DIR / "news.tsv"
    val_behaviors_path = MIND_SMALL_VAL_DATASET_DIR / "behaviors.tsv"

    news_df = read_news_df(val_news_path)
    behavior_df = read_behavior_df(val_behaviors_path)

    logging.info(f"shape: {behavior_df.head(1).shape}")
    logging.info(behavior_df.head(1))

    # Build mapping: news_id -> label (0=fake, 1=real)
    # We assume 'label' column exists (from your converter)
    if "label" not in news_df.columns:
        raise RuntimeError("Expected 'label' column in news.tsv (0=fake, 1=real).")

    # Polars -> python dicts
    news_records = news_df.to_dicts()
    news_label_map = {
        row["news_id"]: int(row["label"]) for row in news_records
    }

    # We also keep behavior rows as python dicts
    behavior_records = behavior_df.to_dicts()

    # 3) Build evaluation dataset (same pipeline as train.py)
    transform_fn = create_transform_fn_from_pretrained_tokenizer(
        PRETRAINED_NAME, MAX_LEN
    )
    eval_dataset = MINDValDataset(
        behavior_df, news_df, transform_fn, history_size=HISTORY_SIZE
    )
    eval_dataloader = DataLoader(
        eval_dataset, batch_size=EVAL_BATCH_SIZE, pin_memory=True
    )

    # 4) Loop over impressions; split into HF (fake clicked) and HR (real clicked)
    hf_metrics_list: list[RecMetrics] = []
    hr_metrics_list: list[RecMetrics] = []

    hf_ratio5_list: list[float] = []
    hf_ratio10_list: list[float] = []
    hr_ratio5_list: list[float] = []
    hr_ratio10_list: list[float] = []

    used_impressions_hf = 0
    used_impressions_hr = 0

    for idx, batch in enumerate(eval_dataloader):
        # match with corresponding behavior row
        row = behavior_records[idx]
        imps = row["impressions"]  # list of {"news_id": ..., "clicked": 0/1}

        # Identify clicked news_id
        clicked_ids = [imp["news_id"] for imp in imps if imp["clicked"] == 1]
        if len(clicked_ids) != 1:
            # Skip weird cases
            logging.warning(
                f"Unexpected number of clicked items at idx={idx}: {len(clicked_ids)}"
            )
            continue
        clicked_nid = clicked_ids[0]

        # Get clicked label (0=fake, 1=real)
        clicked_label = news_label_map.get(clicked_nid, None)
        if clicked_label is None:
            logging.warning(
                f"No label found for clicked news_id={clicked_nid} at idx={idx}. Skipping."
            )
            continue

        # Move tensors to device
        batch["news_histories"] = batch["news_histories"].to(DEVICE)
        batch["candidate_news"] = batch["candidate_news"].to(DEVICE)
        batch["target"] = batch["target"].to(DEVICE)

        # Forward pass
        with torch.no_grad():
            output: ModelOutput = model(**batch)

        # Flatten scores and labels
        y_score: np.ndarray = (
            output.logits.flatten().cpu().to(torch.float64).numpy()
        )
        y_true: np.ndarray = (
            batch["target"].flatten().cpu().to(torch.int64).numpy()
        )

        # Sanity check: same length as impressions list
        TARGET_LEN = 3

        if len(imps) != len(y_score) or len(imps) != len(y_true):
            # Optional: one-time debug, or just remove logging entirely
            logging.debug(
                f"Length mismatch at idx={idx}: "
                f"{len(imps)} candidates vs {len(y_score)} scores. "
                f"Truncating both to {TARGET_LEN}."
            )

        # Always cap everything to 3 so it matches the trained model
        imps = imps[:TARGET_LEN]
        y_score = y_score[:TARGET_LEN]
        y_true = y_true[:TARGET_LEN]

        # Candidate real/fake labels for Ratio@K
        cand_labels = [news_label_map[imp["news_id"]] for imp in imps]

        r5 = ratio_at_k(cand_labels, y_score, k=5)
        r10 = ratio_at_k(cand_labels, y_score, k=10)

        # Per-impression ranking metrics
        metrics = RecEvaluator.evaluate_all(y_true, y_score)

        if clicked_label == 0:
            # HF: clicked fake
            hf_metrics_list.append(metrics)
            hf_ratio5_list.append(r5)
            hf_ratio10_list.append(r10)
            used_impressions_hf += 1
        elif clicked_label == 1:
            # HR: clicked real
            hr_metrics_list.append(metrics)
            hr_ratio5_list.append(r5)
            hr_ratio10_list.append(r10)
            used_impressions_hr += 1

    logging.info(
        f"Used {used_impressions_hf} HF impressions and {used_impressions_hr} HR impressions."
    )

    # 5) Average metrics per group
    hf_avg = average_rec_metrics(hf_metrics_list)
    hr_avg = average_rec_metrics(hr_metrics_list)

    hf_ratio5 = float(np.nanmean(hf_ratio5_list)) if hf_ratio5_list else None
    hf_ratio10 = float(np.nanmean(hf_ratio10_list)) if hf_ratio10_list else None
    hr_ratio5 = float(np.nanmean(hr_ratio5_list)) if hr_ratio5_list else None
    hr_ratio10 = float(np.nanmean(hr_ratio10_list)) if hr_ratio10_list else None

    # 6) Compute RRA (Relative Ranking Advantage, %)
    rra_mrr = rel_adv(
        hr_avg.mrr if hr_avg else None, hf_avg.mrr if hf_avg else None
    )
    rra_ndcg5 = rel_adv(
        hr_avg.ndcg_at_5 if hr_avg else None, hf_avg.ndcg_at_5 if hf_avg else None
    )
    rra_ndcg10 = rel_adv(
        hr_avg.ndcg_at_10 if hr_avg else None, hf_avg.ndcg_at_10 if hf_avg else None
    )
    rra_ratio5 = rel_adv(hr_ratio5, hf_ratio5)
    rra_ratio10 = rel_adv(hr_ratio10, hf_ratio10)

    # 7) Pretty-print table similar to paper
    print()
    print("=== NRMS (GossipCop Val, HF vs HR) ===")
    print("Rec. Type   MRR      nDCG@5   nDCG@10  Ratio@5  Ratio@10")
    print("---------------------------------------------------------")

    def f(x):
        return "  -   " if x is None else f"{x * 100:.2f}" if x <= 1.0 else f"{x:.2f}"

    # HF row (fake clicked)
    print(
        "HF        "
        f"{f(hf_avg.mrr if hf_avg else None):>8}"
        f"{f(hf_avg.ndcg_at_5 if hf_avg else None):>9}"
        f"{f(hf_avg.ndcg_at_10 if hf_avg else None):>9}"
        f"{f(hf_ratio5):>9}"
        f"{f(hf_ratio10):>10}"
    )

    # HR row (real clicked)
    print(
        "HR        "
        f"{f(hr_avg.mrr if hr_avg else None):>8}"
        f"{f(hr_avg.ndcg_at_5 if hr_avg else None):>9}"
        f"{f(hr_avg.ndcg_at_10 if hr_avg else None):>9}"
        f"{f(hr_ratio5):>9}"
        f"{f(hr_ratio10):>10}"
    )

    # RRA row (percentage advantage HR over HF)
    def f_pct(x):
        return "  -   " if x is None else f"{x:.2f}%"

    print(
        "(RRA)     "
        f"{f_pct(rra_mrr):>8}"
        f"{f_pct(rra_ndcg5):>9}"
        f"{f_pct(rra_ndcg10):>9}"
        f"{f_pct(rra_ratio5):>9}"
        f"{f_pct(rra_ratio10):>10}"
    )


if __name__ == "__main__":
    compute_metrics_table()
